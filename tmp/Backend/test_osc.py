import json
import requests
import time
import numpy as np
from collections import defaultdict, Counter
from typing import List, Tuple
import math

# ìš”ì•½ ëª¨ë¸ì„ ìœ„í•œ import
try:
    from transformers import pipeline
    SUMMARIZATION_AVAILABLE = True
except ImportError:
    SUMMARIZATION_AVAILABLE = False
    print("âš ï¸ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ìš”ì•½ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ğŸ” ì‚¬ìš©ì í‚¤ (Wikifier ì‚¬ì´íŠ¸ì—ì„œ ë°œê¸‰)
WIKIFIER_USER_KEY = "dgvbwjfonhpzjpdjjkzpgefbiifvfz"
json_path = "./script/E6DuimPZDz8_ko_transcript.json"
# ========== Step 1: ìë§‰ JSON ë¡œë“œ ==========
def load_youtube_subtitles(json_path: str) -> List[dict]:
    """
    YouTube ìë§‰ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    ìƒˆë¡œìš´ í˜•ì‹ (script_osc.pyì—ì„œ ìƒì„±)ê³¼ ê¸°ì¡´ í˜•ì‹ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ìƒˆë¡œìš´ í˜•ì‹ì¸ì§€ í™•ì¸ (segments í‚¤ê°€ ìˆëŠ”ì§€)
    if 'segments' in data:
        print(f"ğŸ“ ìƒˆë¡œìš´ í˜•ì‹ JSON íŒŒì¼ ë¡œë“œ: {data.get('video_id', 'Unknown')}")
        print(f"ğŸŒ ì–¸ì–´: {data.get('language_code', 'Unknown')}")
        print(f"ğŸ“Š ì´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {data.get('total_segments', 0)}")
        return data['segments']
    else:
        # ê¸°ì¡´ í˜•ì‹ (ì§ì ‘ ë¦¬ìŠ¤íŠ¸)
        print(f"ğŸ“ ê¸°ì¡´ í˜•ì‹ JSON íŒŒì¼ ë¡œë“œ")
        return data

def wikifier(text: str, user_key: str, lang: str = 'en') -> List[Tuple[str, float]]:
    url = 'http://www.wikifier.org/annotate-article'
    params = {
        'userKey': user_key,
        'text': text,
        'lang': lang,
        'support': 'true',
        'pageRankSqThreshold': '0.8',
        'applyPageRankSqThreshold': 'true',
        'nTopDfValuesToIgnore': '100',
        'nWordsToIgnoreFromList': '100',
        'wikiDataClasses': 'false',
        'wikiDataClassIds': 'false',
        'supportText': 'false',
        'ranges': 'false',
        'includeCosines': 'false'
    }

    try:
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        response = requests.post(url, data=params, timeout=30)
        
        if response.status_code == 429:
            print("âš ï¸ Rate limit ì´ˆê³¼ - 60ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„")
            time.sleep(60)
            response = requests.post(url, data=params, timeout=30)
        
        if response.status_code != 200:
            print(f"âŒ Wikifier API ì˜¤ë¥˜ (ìƒíƒœì½”ë“œ: {response.status_code}): {response.text[:200]}")
            return []
        
        annotations = response.json().get("annotations", [])
        top_3 = sorted(
            [(ann['title'], ann['pageRank']) for ann in annotations if 'title' in ann and 'pageRank' in ann],
            key=lambda x: x[1], reverse=True
        )[:3]
        return top_3
        
    except requests.exceptions.Timeout:
        print("âš ï¸ Wikifier API íƒ€ì„ì•„ì›ƒ")
        return []
    except requests.exceptions.ConnectionError:
        print("âš ï¸ Wikifier API ì—°ê²° ì˜¤ë¥˜")
        return []
    except Exception as e:
        print(f"âš ï¸ Wikifier ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def normalize_scores(scores: List[float]) -> List[float]:
    total = sum(scores)
    if total > 0:
        return [s / total for s in scores]
    elif len(scores) > 0:
        return [1/len(scores)] * len(scores)
    else:
        return [1.0]  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê¸°ë³¸ê°’

def shannon_entropy(prob_dist: List[float]) -> float:
    return -sum(p * math.log2(p) for p in prob_dist if p > 0)

def normalize_entropy(entropy_list: List[float]) -> List[float]:
    min_e, max_e = min(entropy_list), max(entropy_list)
    return [(e - min_e) / (max_e - min_e + 1e-8) for e in entropy_list]

def compute_entropy_deltas(norm_entropy: List[float]) -> List[float]:
    return [abs(norm_entropy[i+1] - norm_entropy[i]) for i in range(len(norm_entropy) - 1)]


def segment_blocks_by_entropy(entropy_deltas: List[float], threshold: float = 0.30) -> List[int]:
    """
    ì—”íŠ¸ë¡œí”¼ ë³€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ê²½ê³„ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    ë” ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    boundaries = [0]
    for i, delta in enumerate(entropy_deltas):
        if delta > threshold:
            boundaries.append(i + 1)
    boundaries.append(len(entropy_deltas) + 1)
    
    # ìµœì†Œ ì„¸ê·¸ë¨¼íŠ¸ í¬ê¸° ë³´ì¥ (ë„ˆë¬´ ì‘ì€ ì„¸ê·¸ë¨¼íŠ¸ ë°©ì§€)
    filtered_boundaries = [boundaries[0]]
    for i in range(1, len(boundaries) - 1):
        if boundaries[i] - filtered_boundaries[-1] >= 1:  # ìµœì†Œ 1ë¸”ë¡
            filtered_boundaries.append(boundaries[i])
    filtered_boundaries.append(boundaries[-1])
    
    segments = [(filtered_boundaries[i], filtered_boundaries[i+1]) for i in range(len(filtered_boundaries) - 1)]
    
    print(f"ğŸ” ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• :")
    print(f"   ì„ê³„ê°’: {threshold}")
    print(f"   ìƒì„±ëœ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(segments)}")
    for i, (start, end) in enumerate(segments):
        print(f"   ì„¸ê·¸ë¨¼íŠ¸ {i+1}: ë¸”ë¡ {start}~{end-1} ({end-start}ê°œ ë¸”ë¡)")
    
    return segments


def summarize_segments(blocks, segments, all_topics):
    print("ğŸ“Œ Segment Summary (with Wikifier):")
    for seg_idx, (start, end) in enumerate(segments):
        segment_topics = []
        for i in range(start, end):
            segment_topics.extend([topic for topic, _ in all_topics[i]])
        dominant = Counter(segment_topics).most_common(1)[0][0] if segment_topics else "N/A"
        print(f"Segment {seg_idx + 1}: Blocks {start} ~ {end - 1}, Dominant Topic: {dominant}")
        
def print_detailed_blocks_summary(all_topics: List[List[Tuple[str, float]]], entropy_list: List[float]):
    print(f"{'t':<3} | {'Top-3 Topics (PageRank)':<60} | {'H_t (nat)':>9} | {'Î”H_t':>6}")
    print('-' * 90)

    prev_entropy = None
    for t, (topics, entropy) in enumerate(zip(all_topics, entropy_list), 1):
        topics_str = ', '.join([f"{topic} {score:.2f}" for topic, score in topics])
        delta = abs(entropy - prev_entropy) if prev_entropy is not None else None
        delta_str = f"{delta:.2f}" if delta is not None else "â€”"
        print(f"{t:<3} | {topics_str:<60} | {entropy:>9.2f} | {delta_str:>6}")
        prev_entropy = entropy
        
import pandas as pd

def save_summary_to_csv(all_topics: List[List[Tuple[str, float]]], entropy_list: List[float], output_path: str):
    data = []
    prev_entropy = None

    for t, (topics, entropy) in enumerate(zip(all_topics, entropy_list), 1):
        delta = abs(entropy - prev_entropy) if prev_entropy is not None else None
        topic_strs = [f"{topic} {score:.2f}" for topic, score in topics]
        row = {
            't': t,
            'Top1': topic_strs[0] if len(topic_strs) > 0 else '',
            'Top2': topic_strs[1] if len(topic_strs) > 1 else '',
            'Top3': topic_strs[2] if len(topic_strs) > 2 else '',
            'H_t (nat)': round(entropy, 4),
            'Î”H_t': round(delta, 4) if delta is not None else ''
        }
        data.append(row)
        prev_entropy = entropy

    df = pd.DataFrame(data)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nâœ… CSV ì €ì¥ ì™„ë£Œ: {output_path}")

def save_sorted_blocks_to_json(sorted_blocks: List[str], output_path: str = "osc/sorted_blocks.json"):
    """
    sorted_blocksë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        sorted_blocks: ì €ì¥í•  ë¸”ë¡ ë¦¬ìŠ¤íŠ¸
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: "sorted_blocks.json")
    """
    blocks_data = {
        "total_blocks": len(sorted_blocks),
        "blocks": [
            {
                "block_index": i,
                "text": block.strip(),
                "word_count": len(block.split())
            }
            for i, block in enumerate(sorted_blocks)
        ]
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(blocks_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… sorted_blocks ì €ì¥ ì™„ë£Œ: {output_path} (ì´ {len(sorted_blocks)}ê°œ ë¸”ë¡)")

def load_sorted_blocks_from_json(input_path: str = "osc/sorted_blocks.json") -> List[str]:
    """
    JSON íŒŒì¼ì—ì„œ sorted_blocksë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        input_path: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: "sorted_blocks.json")
    
    Returns:
        ë¸”ë¡ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    blocks = [block["text"] for block in data["blocks"]]
    print(f"âœ… sorted_blocks ë¡œë“œ ì™„ë£Œ: {input_path} (ì´ {len(blocks)}ê°œ ë¸”ë¡)")
    return blocks

def save_segments_to_json(blocks, segments, all_topics, output_path: str = "osc/wikifier_segments.json"):
    """
    Wikifierë¡œ ë‚˜ëˆ ì§„ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì‹œê°„ê³¼ ë‚´ìš©ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        blocks: ì›ë³¸ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸
        segments: ì„¸ê·¸ë¨¼íŠ¸ ê²½ê³„ ì •ë³´ [(start, end), ...]
        all_topics: ê° ë¸”ë¡ì˜ Wikifier í† í”½ ì •ë³´
        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
    """
    segments_data = {
        "total_segments": len(segments),
        "segments": []
    }
    
    for seg_idx, (start, end) in enumerate(segments):
        # ì„¸ê·¸ë¨¼íŠ¸ì˜ ëª¨ë“  ë¸”ë¡ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        segment_text = ""
        for i in range(start, end):
            segment_text += blocks[i] + " "
        
        # ì„¸ê·¸ë¨¼íŠ¸ì˜ ëª¨ë“  í† í”½ ìˆ˜ì§‘
        segment_topics = []
        for i in range(start, end):
            segment_topics.extend([topic for topic, _ in all_topics[i]])
        
        # ì£¼ìš” í† í”½ ì°¾ê¸°
        topic_counter = Counter(segment_topics)
        dominant_topic = topic_counter.most_common(1)[0][0] if segment_topics else "N/A"
        
        # ì‹œê°„ ê³„ì‚° (ê° ë¸”ë¡ì€ 1ë¶„ = 60ì´ˆ)
        start_time_minutes = start
        end_time_minutes = end - 1
        start_time_seconds = start_time_minutes * 60
        end_time_seconds = (end_time_minutes + 1) * 60
        
        # ì‹œê°„ì„ HH:MM:SS í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        def seconds_to_time(seconds):
            hours = seconds // 3600
            minutes = (seconds % 3600) // 60
            secs = seconds % 60
            return f"{hours:02d}:{minutes:02d}:{secs:02d}"
        
        segment_info = {
            "segment_id": seg_idx + 1,
            "block_range": {
                "start_block": start,
                "end_block": end - 1,
                "total_blocks": end - start
            },
            "time_range": {
                "start_time_minutes": start_time_minutes,
                "end_time_minutes": end_time_minutes,
                "start_time_formatted": seconds_to_time(start_time_seconds),
                "end_time_formatted": seconds_to_time(end_time_seconds),
                "duration_minutes": end_time_minutes - start_time_minutes + 1
            },
            "content": {
                "text": segment_text.strip(),
                "word_count": len(segment_text.split()),
                "character_count": len(segment_text.strip())
            },
            "topics": {
                "dominant_topic": dominant_topic,
                "all_topics": list(topic_counter.items()),
                "unique_topics_count": len(topic_counter)
            }
        }
        
        segments_data["segments"].append(segment_info)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(segments_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Wikifier ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ ì™„ë£Œ: {output_path} (ì´ {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")

def generate_summary(text: str, language_code: str = 'ko') -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
    
    Args:
        text (str): ìš”ì•½í•  í…ìŠ¤íŠ¸
        language_code (str): ì–¸ì–´ ì½”ë“œ ('ko' ë˜ëŠ” 'en')
    
    Returns:
        str: ìš”ì•½ëœ í…ìŠ¤íŠ¸
    """
    if not SUMMARIZATION_AVAILABLE:
        return "ìš”ì•½ ìƒì„±ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
    
    try:
        # ì–¸ì–´ì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
        if language_code == 'ko':
            model_name = "digit82/kobart-summarization"  # í•œêµ­ì–´ ìš”ì•½ ëª¨ë¸ (ëŒ€ì•ˆ)
        else:
            model_name = "facebook/bart-large-cnn"  # ì˜ì–´ ìš”ì•½ ëª¨ë¸
        
        summarizer = pipeline(
            "summarization",
            model=model_name,
            tokenizer=model_name,
            device=-1  # CPU ì‚¬ìš© (GPUê°€ ì—†ê±°ë‚˜ ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)
        )
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ëª¨ë¸ ì œí•œ ê³ ë ¤)
        max_input_length = 1024 if language_code == 'ko' else 1024
        if len(text) > max_input_length:
            text = text[:max_input_length]
        
        summary = summarizer(
            text, 
            max_length=130, 
            min_length=30, 
            do_sample=False,
            truncation=True
        )
        return summary[0]['summary_text']
        
    except Exception as e:
        return f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def save_segments_to_txt(blocks, segments, all_topics, output_path: str = "osc/wikifier_segments.txt"):
    """
    Wikifierë¡œ ë‚˜ëˆ ì§„ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì½ê¸° ì‰¬ìš´ TXT íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("=== Wikifier ì„¸ê·¸ë¨¼íŠ¸ ìš”ì•½ ===\n\n")
        
        for seg_idx, (start, end) in enumerate(segments):
            # ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            segment_text = ""
            for i in range(start, end):
                segment_text += blocks[i] + " "
            
            # í† í”½ ìˆ˜ì§‘
            segment_topics = []
            for i in range(start, end):
                segment_topics.extend([topic for topic, _ in all_topics[i]])
            
            dominant_topic = Counter(segment_topics).most_common(1)[0][0] if segment_topics else "N/A"
            
            # ì‹œê°„ ê³„ì‚°
            start_time = f"{start:02d}:00"
            end_time = f"{end-1:02d}:59"
            
            f.write(f"ğŸ“Œ ì„¸ê·¸ë¨¼íŠ¸ {seg_idx + 1}\n")
            f.write(f"â° ì‹œê°„: {start_time} ~ {end_time}\n")
            f.write(f"ğŸ·ï¸  ì£¼ìš” í† í”½: {dominant_topic}\n")
            f.write(f"ğŸ“ ë‚´ìš©:\n{segment_text.strip()}\n")
            f.write("-" * 50 + "\n\n")
    
    print(f"âœ… Wikifier ì„¸ê·¸ë¨¼íŠ¸ TXT ì €ì¥ ì™„ë£Œ: {output_path}")

def save_segments_with_summaries(blocks, segments, all_topics, language_code: str = 'ko', output_path: str = "osc/wikifier_segments_with_summaries.txt"):
    """
    Wikifierë¡œ ë‚˜ëˆ ì§„ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìš”ì•½ê³¼ í•¨ê»˜ TXT íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("=== Wikifier ì„¸ê·¸ë¨¼íŠ¸ ìš”ì•½ (AI ìš”ì•½ í¬í•¨) ===\n\n")
        
        for seg_idx, (start, end) in enumerate(segments):
            # ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            segment_text = ""
            for i in range(start, end):
                segment_text += blocks[i] + " "
            
            segment_text = segment_text.strip()
            
            # í† í”½ ìˆ˜ì§‘
            segment_topics = []
            for i in range(start, end):
                segment_topics.extend([topic for topic, _ in all_topics[i]])
            
            dominant_topic = Counter(segment_topics).most_common(1)[0][0] if segment_topics else "N/A"
            
            # ì‹œê°„ ê³„ì‚°
            start_time = f"{start:02d}:00"
            end_time = f"{end-1:02d}:59"
            
            f.write(f"ğŸ“Œ ì„¸ê·¸ë¨¼íŠ¸ {seg_idx + 1}\n")
            f.write(f"â° ì‹œê°„: {start_time} ~ {end_time}\n")
            f.write(f"ğŸ·ï¸  ì£¼ìš” í† í”½: {dominant_topic}\n")
            f.write(f"ğŸ“ ì›ë³¸ ë‚´ìš©:\n{segment_text}\n\n")
            
            # AI ìš”ì•½ ìƒì„±
            if len(segment_text) > 50:  # ì¶©ë¶„íˆ ê¸´ í…ìŠ¤íŠ¸ë§Œ ìš”ì•½
                print(f"ğŸ¤– ì„¸ê·¸ë¨¼íŠ¸ {seg_idx + 1} ìš”ì•½ ìƒì„± ì¤‘...")
                summary = generate_summary(segment_text, language_code)
                f.write(f"ğŸ¤– AI ìš”ì•½:\n{summary}\n")
            else:
                f.write("ğŸ¤– AI ìš”ì•½: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ìš”ì•½í•˜ì§€ ì•ŠìŒ\n")
            
            f.write("-" * 50 + "\n\n")
    
    print(f"âœ… Wikifier ì„¸ê·¸ë¨¼íŠ¸ ìš”ì•½ TXT ì €ì¥ ì™„ë£Œ: {output_path}")

def segment_blocks_one_minute(data: List[dict]):
    block_map = defaultdict(str)

    for entry in data:
        block_index = int(entry['start'] // 60)
        block_map[block_index] += ' ' + entry['text']

    segments_one_minute = [block_map[i] for i in sorted(block_map.keys())]

    return segments_one_minute

def process_youtube_segments_with_wikifier(json_path: str, user_key: str):
    data = load_youtube_subtitles(json_path)
    
    # ìë§‰ 1ë¶„ë³„ë¡œ ëŠê¸°.
    segments_one_minute = segment_blocks_one_minute(data)

    # Wikifier í˜¸ì¶œ (1ì´ˆ ë”œë ˆì´ë¡œ API ê³¼ë¶€í•˜ ë°©ì§€)
    all_topics = []
    
    # JSON íŒŒì¼ì—ì„œ ì–¸ì–´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    with open(json_path, 'r', encoding='utf-8') as f:
        json_data = json.load(f)
    language_code = json_data.get('language_code', 'en')  # ê¸°ë³¸ê°’ì€ ì˜ì–´
    
    print(f"ğŸŒ Wikifier ì–¸ì–´ ì„¤ì •: {language_code}")
    
    # ë¸”ë¡ ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
    max_blocks = 10  # 10ê°œ ë¸”ë¡ìœ¼ë¡œ ëŠ˜ë¦¼
    print(f"ğŸ”§ ì²˜ë¦¬í•  ë¸”ë¡ ìˆ˜ ì œí•œ: {max_blocks}ê°œ")
    
    for i, block in enumerate(sorted_blocks[:max_blocks]):
        print(f"ğŸ” Wikifier ì²˜ë¦¬ ì¤‘: Block {i+1}/{min(max_blocks, len(sorted_blocks))}")
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ë° ì „ì²˜ë¦¬
        text = block[:1000]  # 1000ìë¡œ ë” ì¤„ì„
        if len(text.strip()) < 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°
            print(f"   â­ï¸ Block {i+1}: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ (ê±´ë„ˆëœ€)")
            all_topics.append([("empty_block", 1.0)])
            continue
            
        topics = wikifier(text, user_key, lang=language_code)
        
        # Wikifier ê²°ê³¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ë”ë¯¸ ë°ì´í„° ì‚¬ìš©
        if not topics:
            print(f"   âš ï¸ Block {i+1}: Wikifier ê²°ê³¼ ì—†ìŒ, ë”ë¯¸ ë°ì´í„° ì‚¬ìš©")
            topics = [("dummy_topic", 1.0)]
        
        all_topics.append(topics)
        
        # ë” ê¸´ ë”œë ˆì´ë¡œ API ê³¼ë¶€í•˜ ë°©ì§€
        time.sleep(3)  # 3ì´ˆ ë”œë ˆì´
        
        # 3ê°œ ë¸”ë¡ë§ˆë‹¤ ì¶”ê°€ ë”œë ˆì´
        if (i + 1) % 3 == 0:
            print(f"   â¸ï¸ 3ê°œ ë¸”ë¡ ì™„ë£Œ, 15ì´ˆ ëŒ€ê¸°...")
            time.sleep(15)
    
    # ì ìˆ˜ ì •ê·œí™” ë° ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
    entropy_list = []
    for topics in all_topics:
        scores = [score for _, score in topics]
        probs = normalize_scores(scores)
        entropy = shannon_entropy(probs)
        entropy_list.append(entropy)
    
    norm_entropy = normalize_entropy(entropy_list)
    entropy_deltas = compute_entropy_deltas(norm_entropy)
    segments = segment_blocks_by_entropy(entropy_deltas)
    print_detailed_blocks_summary(all_topics, entropy_list)


    summarize_segments(sorted_blocks, segments, all_topics)
    
    save_summary_to_csv(all_topics, entropy_list, "osc/segment_summary.csv")
            # Wikifier ì„¸ê·¸ë¨¼íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    save_segments_to_json(sorted_blocks, segments, all_topics)
    save_segments_to_txt(sorted_blocks, segments, all_topics)
    
    # AI ìš”ì•½ê³¼ í•¨ê»˜ ì €ì¥
    save_segments_with_summaries(sorted_blocks, segments, all_topics, language_code)


#process_youtube_segments_with_wikifier(json_path, WIKIFIER_USER_KEY)

test_data = load_youtube_subtitles(json_path)
print(segment_blocks_one_minute(test_data))