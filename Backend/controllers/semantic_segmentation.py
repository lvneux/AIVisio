"""
Semantic Segmentationì„ ì´ìš©í•œ ìë™ ì±•í„° ìƒì„± (ëª©í‘œ ì±•í„° ìˆ˜ ë²”ìœ„ ìë™ì¡°ì • í¬í•¨)
- centroid ê¸°ë°˜ ì£¼ì œ ë³€í™” ê°ì§€
- ì§§ì€ ì±•í„° ë³‘í•©
- ì˜ìƒ ê¸¸ì´ì— ë”°ë¥¸ ëª©í‘œ ì±•í„° ìˆ˜ ë²”ìœ„ ì‚°ì • ë° threshold ì¡°ì •(ë°˜ë³µ íƒìƒ‰)
"""

from typing import List, Tuple, Optional
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
from Backend.models.video_segment import VideoSegment

try:
    from sentence_transformers import SentenceTransformer  # type: ignore
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("âš ï¸ sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install sentence-transformersë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")


def compute_target_chapter_range(video_duration: float) -> Tuple[int, int]:
    """
    ì˜ìƒ ê¸¸ì´(ì´ˆ)ì— ë”°ë¼ ê¶Œì¥ ì±•í„° ìˆ˜ ë²”ìœ„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ê²½í—˜ì  ê·œì¹™ â€” í•„ìš”ì— ë”°ë¼ ì¡°ì •í•˜ì„¸ìš”)
    """
    if not video_duration or video_duration <= 0:
        return (5, 20)

    minutes = video_duration / 60.0

    if minutes <= 1:
        return (1, 3)
    if minutes <= 3:
        return (3, 6)
    if minutes <= 7:
        return (5, 10)
    if minutes <= 12:
        return (8, 18)
    if minutes <= 30:
        return (12, 30)
    if minutes <= 60:
        return (20, 50)
    # ë§¤ìš° ê¸´ ì˜ìƒ
    return (30, 80)


def group_transcripts_by_time(transcript_data, window_seconds: int = 60) -> List[Tuple[float, float, str]]:
    if not transcript_data:
        return []

    grouped_segments = []
    current_window_start = None
    current_window_texts = []
    current_window_end = None

    for idx, transcript in enumerate(transcript_data):
        start_time = float(transcript.start)
        end_time = float(transcript.start + transcript.duration)
        text = transcript.text.strip() if getattr(transcript, "text", None) else ""

        if current_window_start is None:
            current_window_start = start_time
            current_window_end = end_time

        current_window_texts.append(text)
        if current_window_end is not None:
            current_window_end = max(current_window_end, end_time)
        else:
            current_window_end = end_time

        is_last = (idx == len(transcript_data) - 1)
        if (current_window_end - current_window_start >= window_seconds) or is_last:
            combined_text = " ".join([t for t in current_window_texts if t])
            grouped_segments.append((current_window_start, current_window_end, combined_text))
            current_window_start = None
            current_window_end = None
            current_window_texts = []

    return grouped_segments


def calculate_embeddings(text_segments: List[str], model) -> np.ndarray:
    if not text_segments:
        return np.array([])
    embeddings = model.encode(text_segments, show_progress_bar=False)
    return np.array(embeddings)


def detect_topic_changes_centroid(embeddings: np.ndarray,
                                  similarity_threshold: float = 0.75,
                                  min_segment_len: int = 1) -> List[int]:
    n = len(embeddings)
    if n == 0:
        return []
    if n == 1:
        return [0, 0]

    change_points = [0]
    current_centroid = embeddings[0].astype(np.float64).copy()
    current_count = 1

    for i in range(1, n):
        sim = float(cosine_similarity(embeddings[i:i+1], current_centroid.reshape(1, -1))[0][0])
        if sim < similarity_threshold and current_count >= min_segment_len:
            change_points.append(i)
            current_centroid = embeddings[i].astype(np.float64).copy()
            current_count = 1
        else:
            current_count += 1
            current_centroid = (current_centroid * (current_count - 1) + embeddings[i]) / current_count

    if change_points[-1] != n - 1:
        change_points.append(n - 1)

    return change_points


def merge_short_segments(grouped_segments: List[Tuple[float, float, str]],
                         change_points: List[int],
                         min_duration: float = 15.0) -> List[Tuple[int, int]]:
    if not grouped_segments or not change_points:
        return []

    ranges = []
    for i in range(len(change_points) - 1):
        s_idx = change_points[i]
        e_idx = change_points[i + 1]
        ranges.append([s_idx, e_idx])

    i = 0
    while i < len(ranges):
        s_idx, e_idx = ranges[i]
        start_time = grouped_segments[s_idx][0]
        end_time = grouped_segments[e_idx][1]
        duration = end_time - start_time

        if duration < min_duration:
            if i == 0 and len(ranges) > 1:
                ranges[i + 1][0] = ranges[i][0]
                ranges.pop(i)
            elif i == len(ranges) - 1 and len(ranges) > 1:
                ranges[i - 1][1] = ranges[i][1]
                ranges.pop(i)
                i -= 1
            else:
                ranges[i - 1][1] = ranges[i][1]
                ranges.pop(i)
                i -= 1
        else:
            i += 1

    if not ranges:
        return [(0, len(grouped_segments) - 1)]

    merged = [(r[0], r[1]) for r in ranges]
    return merged


def generate_chapter_title(text: str, max_length: int = 50) -> str:
    if not text:
        return "Chapter"

    cleaned_text = text.strip()
    sentences = [s.strip() for s in cleaned_text.split('.') if s.strip()]
    if sentences:
        title = sentences[0]
        if len(title) > max_length:
            title = title[:max_length].rsplit(' ', 1)[0] + "..."
        return title
    if len(cleaned_text) > max_length:
        return cleaned_text[:max_length].rsplit(' ', 1)[0] + "..."
    return cleaned_text


def create_semantic_segments(transcript_data,
                             video_id: str,
                             video_duration: Optional[float] = None,
                             initial_window_seconds: int = 60,
                             desired_min_duration: float = 15.0,
                             initial_similarity_threshold: float = 0.75,
                             max_adjust_iters: int = 6) -> List[VideoSegment]:
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸:
    1) ë™ì  window_seconds ê²°ì •(ì˜ìƒ ê¸¸ì´ì— ë”°ë¼)
    2) grouped_segments ìƒì„±
    3) ì„ë² ë”© ê³„ì‚° (í•œ ë²ˆ)
    4) threshold ì¡°ì • ë°˜ë³µ: detect_topic_changes_centroid -> merge_short_segments
       ëª©í‘œ ì±•í„° ë²”ìœ„(ì˜ìƒ ê¸¸ì´ ê¸°ë°˜)ì— ë“¤ë„ë¡ similarity_thresholdë¥¼ ì¡°ì •
    5) VideoSegment ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    print("ğŸ” Semantic Segmentation (ëª©í‘œ ì±•í„° ë²”ìœ„ ìë™ì¡°ì • í¬í•¨) ì‹œì‘")

    if not SENTENCE_TRANSFORMERS_AVAILABLE:
        raise ImportError("sentence-transformersê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install sentence-transformers")

    if not transcript_data:
        print("âš ï¸ ìë§‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    # 1) ë™ì  ìœˆë„ìš° ê³„ì‚° (video_durationì´ ìˆìœ¼ë©´)
    window_seconds = initial_window_seconds
    if video_duration and video_duration > 0:
        # approximate chunk count: aim for chunk ~15~30ì´ˆ ë‚´ì™¸ (clamp)
        approx_chunks = int(max(10, min(video_duration / 20, 120)))
        window_seconds = max(5, int(video_duration / approx_chunks))
        print(f"ğŸ”§ ë™ì  ìœˆë„ìš° ì ìš©: window_seconds={window_seconds}s (approx_chunks={approx_chunks})")

    # 2) ê·¸ë£¹í•‘
    grouped_segments = group_transcripts_by_time(transcript_data, window_seconds)
    if not grouped_segments:
        print("âš ï¸ grouped_segmentsê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    print(f"ğŸ“Š ì´ˆê¸° êµ¬ê°„ ìˆ˜: {len(grouped_segments)}")

    # 3) ì„ë² ë”© ê³„ì‚° (í•œ ë²ˆë§Œ)
    print("ğŸ¤– Embedding ëª¨ë¸ ë¡œë”© ë° ì„ë² ë”© ê³„ì‚° ì¤‘...")
    model = None
    try:
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # type: ignore
    except Exception as e:
        print(f"âš ï¸ ë‹¤êµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. ì˜ì–´ ëª¨ë¸ë¡œ ëŒ€ì²´ ì‹œë„.")
        try:
            model = SentenceTransformer('all-MiniLM-L6-v2')  # type: ignore
        except Exception as e2:
            print(f"âŒ Embedding ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e2}")
            return []
    
    if model is None:
        print("âŒ Embedding ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    text_segments = [seg[2] for seg in grouped_segments]
    embeddings = calculate_embeddings(text_segments, model)
    if embeddings.size == 0:
        print("âš ï¸ ì„ë² ë”© ê³„ì‚° ì‹¤íŒ¨ ë˜ëŠ” í…ìŠ¤íŠ¸ ë¹„ì–´ìˆìŒ")
        return []
    print(f"âœ… {len(embeddings)} ì„ë² ë”© ì™„ë£Œ")

    # target chapter range ê²°ì •
    min_ch, max_ch = compute_target_chapter_range(video_duration) if video_duration else (5, 20)
    print(f"ğŸ¯ ëª©í‘œ ì±•í„° ë²”ìœ„: {min_ch} ~ {max_ch}")

    # 4) threshold ì¡°ì • ë£¨í”„
    lo_thresh = 0.55
    hi_thresh = 0.92
    best_result = None  # (num_segments, threshold, merged_ranges)
    best_diff = float('inf')

    # ì‹œì‘ threshold
    thresh = initial_similarity_threshold

    for it in range(max_adjust_iters):
        change_points = detect_topic_changes_centroid(embeddings, similarity_threshold=thresh, min_segment_len=1)
        merged_ranges = merge_short_segments(grouped_segments, change_points, min_duration=desired_min_duration)
        num_segments = len(merged_ranges)

        print(f"  ë°˜ë³µ {it+1}: threshold={thresh:.3f} -> segments={num_segments}")

        # ëª©í‘œ ë²”ìœ„ ë‚´ì´ë©´ ë°”ë¡œ ì±„íƒ
        if min_ch <= num_segments <= max_ch:
            best_result = (num_segments, thresh, merged_ranges)
            print("âœ… ëª©í‘œ ë²”ìœ„ ë‚´ì— ë“¤ì—ˆìŠµë‹ˆë‹¤.")
            break

        # ê°€ì¥ ê·¼ì ‘í•œ ê²°ê³¼ ì €ì¥
        diff = min(abs(num_segments - min_ch), abs(num_segments - max_ch))
        if diff < best_diff:
            best_diff = diff
            best_result = (num_segments, thresh, merged_ranges)

        # threshold ì¡°ì • ì „ëµ:
        # - segmentsê°€ ë„ˆë¬´ ë§ìœ¼ë©´(ê³¼ë¶„í• ): threshold ë‚®ì¶°ì„œ ë³‘í•©ì„ ìœ ë„ (sim < thresh ê°€ ë¶„í•  ì¡°ê±´ì´ë¯€ë¡œ ë‚®ì¶”ë©´ ëœ ë¶„í• )
        # - segmentsê°€ ë„ˆë¬´ ì ìœ¼ë©´(ê³¼ì†Œë¶„í• ): threshold ë†’ì—¬ì„œ ë” ë¶„í• 
        if num_segments > max_ch:
            # ë„ˆë¬´ ë§ìŒ -> ë‚®ì¶°ì•¼ í•¨
            hi_thresh = thresh
            thresh = (thresh + lo_thresh) / 2.0
        elif num_segments < min_ch:
            # ë„ˆë¬´ ì ìŒ -> ë†’ì—¬ì•¼ í•¨
            lo_thresh = thresh
            thresh = (thresh + hi_thresh) / 2.0

    if best_result is None:
        print("âš ï¸ ëª©í‘œ ë²”ìœ„ì— ë„ë‹¬í•˜ì§€ ëª»í–ˆì§€ë§Œ ê°€ì¥ ê·¼ì ‘í•œ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # fallback: compute once more with initial threshold if none
        change_points = detect_topic_changes_centroid(embeddings, similarity_threshold=initial_similarity_threshold, min_segment_len=1)
        merged_ranges = merge_short_segments(grouped_segments, change_points, min_duration=desired_min_duration)
        best_result = (len(merged_ranges), initial_similarity_threshold, merged_ranges)

    final_num, final_thresh, final_ranges = best_result
    print(f"ğŸ”š ìµœì¢… ì„ íƒ: threshold={final_thresh:.3f}, ì±•í„°ìˆ˜={final_num}")

    # 5) VideoSegment ê°ì²´ ìƒì„±
    video_segments: List[VideoSegment] = []
    for i, (s_idx, e_idx) in enumerate(final_ranges):
        seg_start = grouped_segments[s_idx][0]
        seg_end = grouped_segments[e_idx][1]
        segment_texts = [grouped_segments[j][2] for j in range(s_idx, e_idx + 1)]
        combined_text = " ".join([t for t in segment_texts if t])
        chapter_title = generate_chapter_title(combined_text, max_length=50)

        segment = VideoSegment(
            id=f"{video_id}_seg_{i}",
            video_id=video_id,
            title=chapter_title,
            start_time=seg_start,
            end_time=seg_end,
            subtitles=combined_text,
            tags=[],
            keywords=[],
            summary=(combined_text[:200] + "...") if len(combined_text) > 200 else combined_text,
            cognitive_level="Unknown",
            dok_level="Unknown"
        )
        video_segments.append(segment)
        print(f"   - ìƒì„±: {chapter_title} ({seg_start:.1f}s - {seg_end:.1f}s)")

    print(f"âœ… ì´ {len(video_segments)}ê°œì˜ ì±•í„° ìƒì„± ì™„ë£Œ (threshold={final_thresh:.3f})")
    return video_segments
