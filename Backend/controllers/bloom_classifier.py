"""
Bloom ì¸ì§€ë‹¨ê³„ ë¶„ë¥˜ë¥¼ ìœ„í•œ BloomBERT ëª¨ë¸ ì»¨íŠ¸ë¡¤ëŸ¬
"""

import torch
from transformers import DistilBertTokenizer, DistilBertModel
import torch.nn as nn
from pathlib import Path

# Bloom ì¸ì§€ë‹¨ê³„ ë§¤í•‘
BLOOM_CATEGORIES = {
    0: "Remember",
    1: "Understand", 
    2: "Apply",
    3: "Analyse",
    4: "Evaluate",
    5: "Create"
}

class BloomBERT(nn.Module):
    """Bloom ì¸ì§€ë‹¨ê³„ ë¶„ë¥˜ë¥¼ ìœ„í•œ BERT ëª¨ë¸"""
    
    def __init__(self, output_dim=6):
        super().__init__()
        self.bert = DistilBertModel.from_pretrained("distilbert-base-uncased")
        self.dropout = nn.Dropout(0.3)
        self.attention = nn.Linear(self.bert.config.hidden_size, 1)

        # two layer classifier
        self.classifier = nn.Sequential(
            nn.Linear(self.bert.config.hidden_size, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, output_dim),
        )

    def forward(self, input_ids, attention_mask=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state  # [batch, seq_len, hidden]

        # attention pooling
        attn_weights = torch.softmax(self.attention(hidden_states).squeeze(-1), dim=1)
        pooled_output = torch.sum(hidden_states * attn_weights.unsqueeze(-1), dim=1)

        x = self.dropout(pooled_output)
        logits = self.classifier(x)
        return logits

class BloomClassifier:
    """Bloom ì¸ì§€ë‹¨ê³„ ë¶„ë¥˜ê¸°"""
    
    def __init__(self, model_path=None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        if model_path is None:
            # ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ
            root_dir = Path(__file__).resolve().parents[1]
            model_path = root_dir / "models" / "bloombert_model.pt"
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = BloomBERT()
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.to(self.device)
        self.model.eval()
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
        
        print(f"âœ… BloomBERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: {self.device})")
    
    def predict_bloom_category(self, text):
        """í…ìŠ¤íŠ¸ì˜ Bloom ì¸ì§€ë‹¨ê³„ë¥¼ ì˜ˆì¸¡"""
        try:
            # í…ìŠ¤íŠ¸ í† í°í™”
            encodings = self.tokenizer(text, truncation=True, padding=True, return_tensors="pt")
            input_ids = encodings["input_ids"].to(self.device)
            attention_mask = encodings["attention_mask"].to(self.device)
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            with torch.no_grad():
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                pred_class = int(torch.argmax(outputs, dim=1).cpu().numpy()[0])
            
            return BLOOM_CATEGORIES[pred_class]
            
        except Exception as e:
            print(f"âš ï¸ Bloom ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {e}")
            return "Unknown"
    
    def predict_segments(self, segments):
        """ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ì˜ ê° ìë§‰ì— ëŒ€í•´ Bloom ë¶„ë¥˜ ìˆ˜í–‰"""
        print(f"\nğŸ§  Bloom ì¸ì§€ë‹¨ê³„ ë¶„ë¥˜ ì‹œì‘...")
        
        for i, segment in enumerate(segments):
            if hasattr(segment, 'subtitles') and segment.subtitles:
                # ìë§‰ í…ìŠ¤íŠ¸ë¡œ Bloom ë¶„ë¥˜
                bloom_category = self.predict_bloom_category(segment.subtitles)
                segment.bloom_category = bloom_category
                
                print(f"   ì„¸ê·¸ë¨¼íŠ¸ {i+1}: {bloom_category}")
            else:
                segment.bloom_category = "Unknown"
                print(f"   ì„¸ê·¸ë¨¼íŠ¸ {i+1}: ìë§‰ ì—†ìŒ")
        
        print("âœ… Bloom ë¶„ë¥˜ ì™„ë£Œ!")
        return segments
