import json
import nltk
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
from konlpy.tag import Okt

# NLTK ì´ˆê¸°í™”
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')

# Konlpy í˜•íƒœì†Œ ë¶„ì„ê¸°
okt = Okt()

# ì–¸ì–´ì— ë”°ë¥¸ ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜
def extract_nouns(text, lang='en'):
    if lang == 'ko':
        return okt.nouns(text)
    else:
        words = word_tokenize(text)
        tagged = pos_tag(words)
        nouns = [word for word, pos in tagged if pos.startswith('NN')]
        stop_words = set(stopwords.words('english'))
        return [word for word in nouns if word.lower() not in stop_words]

# ìë§‰ ë° ëª…ì‚¬ ì¶”ì¶œ
def process_video(video_id):
    try:
        # ìë§‰ ëª©ë¡ í™•ì¸
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)

        # ì–¸ì–´ ìš°ì„ ìˆœìœ„ ì„¤ì •: en > ko
        try:
            transcript = transcript_list.find_transcript(['en']).fetch()
            lang = 'en'
            print("ğŸ“Œ ì˜ì–´ ìë§‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except:
            transcript = transcript_list.find_transcript(['ko']).fetch()
            lang = 'ko'
            print("ğŸ“Œ í•œêµ­ì–´ ìë§‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        transcript_data = transcript.to_raw_data()

        # ìë§‰ JSON ì €ì¥
        with open(f'./script/{video_id}_subtitles.json', 'w', encoding='utf-8') as f:
            json.dump(transcript_data, f, ensure_ascii=False, indent=4)
        print("âœ… ìë§‰ ì¶”ì¶œì´ ì™„ë£Œë˜ì–´ 'subtitles.json'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ìë§‰ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ
        noun_data = []
        for entry in transcript_data:
            nouns = extract_nouns(entry['text'], lang=lang)
            noun_data.append({
                'start': entry['start'],
                'end': entry['start'] + entry['duration'],
                'nouns': nouns
            })

        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open('./script/noun.json', 'w', encoding='utf-8') as f:
            json.dump(noun_data, f, ensure_ascii=False, indent=4)

        print("âœ… ëª…ì‚¬ ì¶”ì¶œì´ ì™„ë£Œë˜ì–´ 'noun.json'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except TranscriptsDisabled:
        print("âŒ ì´ ì˜ìƒì—ëŠ” ìë§‰ì´ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
video_id = 'aircAruvnKk'  # ì›í•˜ëŠ” ì˜ìƒ ID
process_video(video_id)
