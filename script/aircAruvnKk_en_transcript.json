{
  "video_id": "aircAruvnKk",
  "language_code": "en",
  "total_segments": 286,
  "extraction_time": "2025-07-25 11:43:36.328677",
  "segments": [
    {
      "start": 4.22,
      "duration": 1.18,
      "end": 5.3999999999999995,
      "text": "This is a 3."
    },
    {
      "start": 6.06,
      "duration": 4.653,
      "end": 10.713,
      "text": "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels,"
    },
    {
      "start": 10.713,
      "duration": 3.007,
      "end": 13.719999999999999,
      "text": "but your brain has no trouble recognizing it as a 3."
    },
    {
      "start": 14.34,
      "duration": 2.219,
      "end": 16.559,
      "text": "And I want you to take a moment to appreciate how"
    },
    {
      "start": 16.559,
      "duration": 2.401,
      "end": 18.96,
      "text": "crazy it is that brains can do this so effortlessly."
    },
    {
      "start": 19.7,
      "duration": 3.262,
      "end": 22.962,
      "text": "I mean, this, this and this are also recognizable as 3s,"
    },
    {
      "start": 22.962,
      "duration": 4.251,
      "end": 27.213,
      "text": "even though the specific values of each pixel is very different from one"
    },
    {
      "start": 27.213,
      "duration": 1.107,
      "end": 28.32,
      "text": "image to the next."
    },
    {
      "start": 28.9,
      "duration": 4.048,
      "end": 32.948,
      "text": "The particular light-sensitive cells in your eye that are firing when you"
    },
    {
      "start": 32.948,
      "duration": 3.992,
      "end": 36.94,
      "text": "see this 3 are very different from the ones firing when you see this 3."
    },
    {
      "start": 37.52,
      "duration": 5.22,
      "end": 42.74,
      "text": "But something in that crazy-smart visual cortex of yours resolves these as representing"
    },
    {
      "start": 42.74,
      "duration": 5.1,
      "end": 47.84,
      "text": "the same idea, while at the same time recognizing other images as their own distinct"
    },
    {
      "start": 47.84,
      "duration": 0.5,
      "end": 48.34,
      "text": "ideas."
    },
    {
      "start": 49.22,
      "duration": 5.414,
      "end": 54.634,
      "text": "But if I told you, hey, sit down and write for me a program that takes in a grid of"
    },
    {
      "start": 54.634,
      "duration": 4.501,
      "end": 59.135,
      "text": "28x28 pixels like this and outputs a single number between 0 and 10,"
    },
    {
      "start": 59.135,
      "duration": 5.61,
      "end": 64.745,
      "text": "telling you what it thinks the digit is, well the task goes from comically trivial to"
    },
    {
      "start": 64.745,
      "duration": 1.435,
      "end": 66.18,
      "text": "dauntingly difficult."
    },
    {
      "start": 67.16,
      "duration": 3.697,
      "end": 70.857,
      "text": "Unless you've been living under a rock, I think I hardly need to motivate the relevance"
    },
    {
      "start": 70.857,
      "duration": 3.783,
      "end": 74.64,
      "text": "and importance of machine learning and neural networks to the present and to the future."
    },
    {
      "start": 75.12,
      "duration": 3.83,
      "end": 78.95,
      "text": "But what I want to do here is show you what a neural network actually is,"
    },
    {
      "start": 78.95,
      "duration": 3.306,
      "end": 82.256,
      "text": "assuming no background, and to help visualize what it's doing,"
    },
    {
      "start": 82.256,
      "duration": 2.204,
      "end": 84.46,
      "text": "not as a buzzword but as a piece of math."
    },
    {
      "start": 85.02,
      "duration": 3.757,
      "end": 88.777,
      "text": "My hope is that you come away feeling like the structure itself is motivated,"
    },
    {
      "start": 88.777,
      "duration": 2.684,
      "end": 91.461,
      "text": "and to feel like you know what it means when you read,"
    },
    {
      "start": 91.461,
      "duration": 2.879,
      "end": 94.34,
      "text": "or you hear about a neural network quote-unquote learning."
    },
    {
      "start": 95.36,
      "duration": 2.901,
      "end": 98.261,
      "text": "This video is just going to be devoted to the structure component of that,"
    },
    {
      "start": 98.261,
      "duration": 1.999,
      "end": 100.25999999999999,
      "text": "and the following one is going to tackle learning."
    },
    {
      "start": 100.96,
      "duration": 2.318,
      "end": 103.27799999999999,
      "text": "What we're going to do is put together a neural"
    },
    {
      "start": 103.278,
      "duration": 2.762,
      "end": 106.04,
      "text": "network that can learn to recognize handwritten digits."
    },
    {
      "start": 109.36,
      "duration": 2.7,
      "end": 112.06,
      "text": "This is a somewhat classic example for introducing the topic,"
    },
    {
      "start": 112.06,
      "duration": 2.168,
      "end": 114.22800000000001,
      "text": "and I'm happy to stick with the status quo here,"
    },
    {
      "start": 114.228,
      "duration": 3.275,
      "end": 117.503,
      "text": "because at the end of the two videos I want to point you to a couple good"
    },
    {
      "start": 117.503,
      "duration": 3.408,
      "end": 120.911,
      "text": "resources where you can learn more, and where you can download the code that"
    },
    {
      "start": 120.911,
      "duration": 2.169,
      "end": 123.08,
      "text": "does this and play with it on your own computer."
    },
    {
      "start": 125.04,
      "duration": 2.621,
      "end": 127.661,
      "text": "There are many many variants of neural networks,"
    },
    {
      "start": 127.661,
      "duration": 4.585,
      "end": 132.246,
      "text": "and in recent years there's been sort of a boom in research towards these variants,"
    },
    {
      "start": 132.246,
      "duration": 4.696,
      "end": 136.942,
      "text": "but in these two introductory videos you and I are just going to look at the simplest"
    },
    {
      "start": 136.942,
      "duration": 2.238,
      "end": 139.18,
      "text": "plain vanilla form with no added frills."
    },
    {
      "start": 139.86,
      "duration": 4.03,
      "end": 143.89000000000001,
      "text": "This is kind of a necessary prerequisite for understanding any of the more powerful"
    },
    {
      "start": 143.89,
      "duration": 4.322,
      "end": 148.212,
      "text": "modern variants, and trust me it still has plenty of complexity for us to wrap our minds"
    },
    {
      "start": 148.212,
      "duration": 0.5,
      "end": 148.712,
      "text": "around."
    },
    {
      "start": 149.12,
      "duration": 4.075,
      "end": 153.195,
      "text": "But even in this simplest form it can learn to recognize handwritten digits,"
    },
    {
      "start": 153.195,
      "duration": 3.325,
      "end": 156.51999999999998,
      "text": "which is a pretty cool thing for a computer to be able to do."
    },
    {
      "start": 157.48,
      "duration": 2.327,
      "end": 159.807,
      "text": "And at the same time you'll see how it does fall"
    },
    {
      "start": 159.807,
      "duration": 2.473,
      "end": 162.28,
      "text": "short of a couple hopes that we might have for it."
    },
    {
      "start": 163.38,
      "duration": 5.12,
      "end": 168.5,
      "text": "As the name suggests neural networks are inspired by the brain, but let's break that down."
    },
    {
      "start": 168.52,
      "duration": 3.14,
      "end": 171.66,
      "text": "What are the neurons, and in what sense are they linked together?"
    },
    {
      "start": 172.5,
      "duration": 5.521,
      "end": 178.021,
      "text": "Right now when I say neuron all I want you to think about is a thing that holds a number,"
    },
    {
      "start": 178.021,
      "duration": 2.419,
      "end": 180.44,
      "text": "specifically a number between 0 and 1."
    },
    {
      "start": 180.68,
      "duration": 1.88,
      "end": 182.56,
      "text": "It's really not more than that."
    },
    {
      "start": 183.78,
      "duration": 5.042,
      "end": 188.822,
      "text": "For example the network starts with a bunch of neurons corresponding to"
    },
    {
      "start": 188.822,
      "duration": 5.398,
      "end": 194.22,
      "text": "each of the 28x28 pixels of the input image, which is 784 neurons in total."
    },
    {
      "start": 194.7,
      "duration": 4.714,
      "end": 199.414,
      "text": "Each one of these holds a number that represents the grayscale value of the"
    },
    {
      "start": 199.414,
      "duration": 4.966,
      "end": 204.38,
      "text": "corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels."
    },
    {
      "start": 205.3,
      "duration": 2.953,
      "end": 208.25300000000001,
      "text": "This number inside the neuron is called its activation,"
    },
    {
      "start": 208.253,
      "duration": 4.35,
      "end": 212.60299999999998,
      "text": "and the image you might have in mind here is that each neuron is lit up when its"
    },
    {
      "start": 212.603,
      "duration": 1.557,
      "end": 214.16,
      "text": "activation is a high number."
    },
    {
      "start": 216.72,
      "duration": 5.14,
      "end": 221.85999999999999,
      "text": "So all of these 784 neurons make up the first layer of our network."
    },
    {
      "start": 226.5,
      "duration": 2.926,
      "end": 229.426,
      "text": "Now jumping over to the last layer, this has 10 neurons,"
    },
    {
      "start": 229.426,
      "duration": 1.934,
      "end": 231.35999999999999,
      "text": "each representing one of the digits."
    },
    {
      "start": 232.04,
      "duration": 4.576,
      "end": 236.61599999999999,
      "text": "The activation in these neurons, again some number that's between 0 and 1,"
    },
    {
      "start": 236.616,
      "duration": 5.504,
      "end": 242.12,
      "text": "represents how much the system thinks that a given image corresponds with a given digit."
    },
    {
      "start": 243.04,
      "duration": 3.381,
      "end": 246.421,
      "text": "There's also a couple layers in between called the hidden layers,"
    },
    {
      "start": 246.421,
      "duration": 3.434,
      "end": 249.855,
      "text": "which for the time being should just be a giant question mark for"
    },
    {
      "start": 249.855,
      "duration": 3.745,
      "end": 253.6,
      "text": "how on earth this process of recognizing digits is going to be handled."
    },
    {
      "start": 254.26,
      "duration": 3.6,
      "end": 257.86,
      "text": "In this network I chose two hidden layers, each one with 16 neurons,"
    },
    {
      "start": 257.86,
      "duration": 2.7,
      "end": 260.56,
      "text": "and admittedly that's kind of an arbitrary choice."
    },
    {
      "start": 261.02,
      "duration": 3.635,
      "end": 264.655,
      "text": "To be honest I chose two layers based on how I want to motivate the structure in"
    },
    {
      "start": 264.655,
      "duration": 3.545,
      "end": 268.2,
      "text": "just a moment, and 16, well that was just a nice number to fit on the screen."
    },
    {
      "start": 268.78,
      "duration": 3.56,
      "end": 272.34,
      "text": "In practice there is a lot of room for experiment with a specific structure here."
    },
    {
      "start": 273.02,
      "duration": 2.647,
      "end": 275.667,
      "text": "The way the network operates, activations in one"
    },
    {
      "start": 275.667,
      "duration": 2.813,
      "end": 278.47999999999996,
      "text": "layer determine the activations of the next layer."
    },
    {
      "start": 279.2,
      "duration": 4.611,
      "end": 283.811,
      "text": "And of course the heart of the network as an information processing mechanism comes down"
    },
    {
      "start": 283.811,
      "duration": 4.402,
      "end": 288.21299999999997,
      "text": "to exactly how those activations from one layer bring about activations in the next"
    },
    {
      "start": 288.213,
      "duration": 0.5,
      "end": 288.713,
      "text": "layer."
    },
    {
      "start": 289.14,
      "duration": 4.493,
      "end": 293.633,
      "text": "It's meant to be loosely analogous to how in biological networks of neurons,"
    },
    {
      "start": 293.633,
      "duration": 3.547,
      "end": 297.18,
      "text": "some groups of neurons firing cause certain others to fire."
    },
    {
      "start": 298.12,
      "duration": 3.461,
      "end": 301.581,
      "text": "Now the network I'm showing here has already been trained to recognize digits,"
    },
    {
      "start": 301.581,
      "duration": 1.819,
      "end": 303.40000000000003,
      "text": "and let me show you what I mean by that."
    },
    {
      "start": 303.64,
      "duration": 4.654,
      "end": 308.294,
      "text": "It means if you feed in an image, lighting up all 784 neurons of the input layer"
    },
    {
      "start": 308.294,
      "duration": 3.257,
      "end": 311.551,
      "text": "according to the brightness of each pixel in the image,"
    },
    {
      "start": 311.551,
      "duration": 4.654,
      "end": 316.205,
      "text": "that pattern of activations causes some very specific pattern in the next layer"
    },
    {
      "start": 316.205,
      "duration": 2.734,
      "end": 318.93899999999996,
      "text": "which causes some pattern in the one after it,"
    },
    {
      "start": 318.939,
      "duration": 3.141,
      "end": 322.08000000000004,
      "text": "which finally gives some pattern in the output layer."
    },
    {
      "start": 322.56,
      "duration": 3.957,
      "end": 326.517,
      "text": "And the brightest neuron of that output layer is the network's choice,"
    },
    {
      "start": 326.517,
      "duration": 2.883,
      "end": 329.4,
      "text": "so to speak, for what digit this image represents."
    },
    {
      "start": 332.56,
      "duration": 3.777,
      "end": 336.337,
      "text": "And before jumping into the math for how one layer influences the next,"
    },
    {
      "start": 336.337,
      "duration": 3.725,
      "end": 340.062,
      "text": "or how training works, let's just talk about why it's even reasonable"
    },
    {
      "start": 340.062,
      "duration": 3.458,
      "end": 343.52000000000004,
      "text": "to expect a layered structure like this to behave intelligently."
    },
    {
      "start": 344.06,
      "duration": 1.16,
      "end": 345.22,
      "text": "What are we expecting here?"
    },
    {
      "start": 345.4,
      "duration": 2.2,
      "end": 347.59999999999997,
      "text": "What is the best hope for what those middle layers might be doing?"
    },
    {
      "start": 348.92,
      "duration": 4.6,
      "end": 353.52000000000004,
      "text": "Well, when you or I recognize digits, we piece together various components."
    },
    {
      "start": 354.2,
      "duration": 2.62,
      "end": 356.82,
      "text": "A 9 has a loop up top and a line on the right."
    },
    {
      "start": 357.38,
      "duration": 3.8,
      "end": 361.18,
      "text": "An 8 also has a loop up top, but it's paired with another loop down low."
    },
    {
      "start": 361.98,
      "duration": 4.84,
      "end": 366.82,
      "text": "A 4 basically breaks down into three specific lines, and things like that."
    },
    {
      "start": 367.6,
      "duration": 3.958,
      "end": 371.55800000000005,
      "text": "Now in a perfect world, we might hope that each neuron in the second"
    },
    {
      "start": 371.558,
      "duration": 3.434,
      "end": 374.992,
      "text": "to last layer corresponds with one of these subcomponents,"
    },
    {
      "start": 374.992,
      "duration": 3.492,
      "end": 378.48400000000004,
      "text": "that anytime you feed in an image with, say, a loop up top,"
    },
    {
      "start": 378.484,
      "duration": 3.899,
      "end": 382.383,
      "text": "like a 9 or an 8, there's some specific neuron whose activation is"
    },
    {
      "start": 382.383,
      "duration": 1.397,
      "end": 383.78,
      "text": "going to be close to 1."
    },
    {
      "start": 384.5,
      "duration": 2.406,
      "end": 386.906,
      "text": "And I don't mean this specific loop of pixels,"
    },
    {
      "start": 386.906,
      "duration": 4.654,
      "end": 391.56,
      "text": "the hope would be that any generally loopy pattern towards the top sets off this neuron."
    },
    {
      "start": 392.44,
      "duration": 3.609,
      "end": 396.049,
      "text": "That way, going from the third layer to the last one just requires"
    },
    {
      "start": 396.049,
      "duration": 3.991,
      "end": 400.03999999999996,
      "text": "learning which combination of subcomponents corresponds to which digits."
    },
    {
      "start": 401.0,
      "duration": 2.199,
      "end": 403.199,
      "text": "Of course, that just kicks the problem down the road,"
    },
    {
      "start": 403.199,
      "duration": 2.2,
      "end": 405.399,
      "text": "because how would you recognize these subcomponents,"
    },
    {
      "start": 405.399,
      "duration": 2.241,
      "end": 407.64,
      "text": "or even learn what the right subcomponents should be?"
    },
    {
      "start": 408.06,
      "duration": 3.158,
      "end": 411.218,
      "text": "And I still haven't even talked about how one layer influences the next,"
    },
    {
      "start": 411.218,
      "duration": 1.842,
      "end": 413.06,
      "text": "but run with me on this one for a moment."
    },
    {
      "start": 413.68,
      "duration": 3.0,
      "end": 416.68,
      "text": "Recognizing a loop can also break down into subproblems."
    },
    {
      "start": 417.28,
      "duration": 2.611,
      "end": 419.89099999999996,
      "text": "One reasonable way to do this would be to first"
    },
    {
      "start": 419.891,
      "duration": 2.889,
      "end": 422.78000000000003,
      "text": "recognize the various little edges that make it up."
    },
    {
      "start": 423.78,
      "duration": 4.619,
      "end": 428.399,
      "text": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7,"
    },
    {
      "start": 428.399,
      "duration": 5.033,
      "end": 433.432,
      "text": "is really just a long edge, or maybe you think of it as a certain pattern of several"
    },
    {
      "start": 433.432,
      "duration": 0.888,
      "end": 434.32,
      "text": "smaller edges."
    },
    {
      "start": 435.14,
      "duration": 3.668,
      "end": 438.808,
      "text": "So maybe our hope is that each neuron in the second layer of"
    },
    {
      "start": 438.808,
      "duration": 3.912,
      "end": 442.71999999999997,
      "text": "the network corresponds with the various relevant little edges."
    },
    {
      "start": 443.54,
      "duration": 3.971,
      "end": 447.511,
      "text": "Maybe when an image like this one comes in, it lights up all of the"
    },
    {
      "start": 447.511,
      "duration": 3.674,
      "end": 451.185,
      "text": "neurons associated with around 8 to 10 specific little edges,"
    },
    {
      "start": 451.185,
      "duration": 3.971,
      "end": 455.156,
      "text": "which in turn lights up the neurons associated with the upper loop"
    },
    {
      "start": 455.156,
      "duration": 4.564,
      "end": 459.72,
      "text": "and a long vertical line, and those light up the neuron associated with a 9."
    },
    {
      "start": 460.68,
      "duration": 4.003,
      "end": 464.683,
      "text": "Whether or not this is what our final network actually does is another question,"
    },
    {
      "start": 464.683,
      "duration": 3.253,
      "end": 467.936,
      "text": "one that I'll come back to once we see how to train the network,"
    },
    {
      "start": 467.936,
      "duration": 4.054,
      "end": 471.98999999999995,
      "text": "but this is a hope that we might have, a sort of goal with the layered structure"
    },
    {
      "start": 471.99,
      "duration": 0.55,
      "end": 472.54,
      "text": "like this."
    },
    {
      "start": 473.16,
      "duration": 3.596,
      "end": 476.75600000000003,
      "text": "Moreover, you can imagine how being able to detect edges and patterns"
    },
    {
      "start": 476.756,
      "duration": 3.544,
      "end": 480.29999999999995,
      "text": "like this would be really useful for other image recognition tasks."
    },
    {
      "start": 480.88,
      "duration": 3.132,
      "end": 484.012,
      "text": "And even beyond image recognition, there are all sorts of intelligent"
    },
    {
      "start": 484.012,
      "duration": 3.268,
      "end": 487.28,
      "text": "things you might want to do that break down into layers of abstraction."
    },
    {
      "start": 488.04,
      "duration": 4.689,
      "end": 492.72900000000004,
      "text": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds,"
    },
    {
      "start": 492.729,
      "duration": 3.774,
      "end": 496.503,
      "text": "which combine to make certain syllables, which combine to form words,"
    },
    {
      "start": 496.503,
      "duration": 3.557,
      "end": 500.06,
      "text": "which combine to make up phrases and more abstract thoughts, etc."
    },
    {
      "start": 501.1,
      "duration": 2.585,
      "end": 503.685,
      "text": "But getting back to how any of this actually works,"
    },
    {
      "start": 503.685,
      "duration": 4.258,
      "end": 507.943,
      "text": "picture yourself right now designing how exactly the activations in one layer might"
    },
    {
      "start": 507.943,
      "duration": 1.977,
      "end": 509.91999999999996,
      "text": "determine the activations in the next."
    },
    {
      "start": 510.86,
      "duration": 5.128,
      "end": 515.988,
      "text": "The goal is to have some mechanism that could conceivably combine pixels into edges,"
    },
    {
      "start": 515.988,
      "duration": 2.992,
      "end": 518.98,
      "text": "or edges into patterns, or patterns into digits."
    },
    {
      "start": 519.44,
      "duration": 3.828,
      "end": 523.268,
      "text": "And to zoom in on one very specific example, let's say the hope"
    },
    {
      "start": 523.268,
      "duration": 3.646,
      "end": 526.914,
      "text": "is for one particular neuron in the second layer to pick up"
    },
    {
      "start": 526.914,
      "duration": 3.706,
      "end": 530.62,
      "text": "on whether or not the image has an edge in this region here."
    },
    {
      "start": 531.44,
      "duration": 3.66,
      "end": 535.1,
      "text": "The question at hand is what parameters should the network have?"
    },
    {
      "start": 535.64,
      "duration": 4.01,
      "end": 539.65,
      "text": "What dials and knobs should you be able to tweak so that it's expressive"
    },
    {
      "start": 539.65,
      "duration": 4.009,
      "end": 543.659,
      "text": "enough to potentially capture this pattern, or any other pixel pattern,"
    },
    {
      "start": 543.659,
      "duration": 4.121,
      "end": 547.78,
      "text": "or the pattern that several edges can make a loop, and other such things?"
    },
    {
      "start": 548.72,
      "duration": 3.094,
      "end": 551.8140000000001,
      "text": "Well, what we'll do is assign a weight to each one of the"
    },
    {
      "start": 551.814,
      "duration": 3.746,
      "end": 555.56,
      "text": "connections between our neuron and the neurons from the first layer."
    },
    {
      "start": 556.32,
      "duration": 1.38,
      "end": 557.7,
      "text": "These weights are just numbers."
    },
    {
      "start": 558.54,
      "duration": 3.358,
      "end": 561.8979999999999,
      "text": "Then take all of those activations from the first layer"
    },
    {
      "start": 561.898,
      "duration": 3.602,
      "end": 565.5,
      "text": "and compute their weighted sum according to these weights."
    },
    {
      "start": 567.7,
      "duration": 3.397,
      "end": 571.0970000000001,
      "text": "I find it helpful to think of these weights as being organized into a"
    },
    {
      "start": 571.097,
      "duration": 3.545,
      "end": 574.6419999999999,
      "text": "little grid of their own, and I'm going to use green pixels to indicate"
    },
    {
      "start": 574.642,
      "duration": 3.101,
      "end": 577.743,
      "text": "positive weights, and red pixels to indicate negative weights,"
    },
    {
      "start": 577.743,
      "duration": 4.037,
      "end": 581.7800000000001,
      "text": "where the brightness of that pixel is some loose depiction of the weight's value."
    },
    {
      "start": 582.78,
      "duration": 3.747,
      "end": 586.5269999999999,
      "text": "Now if we made the weights associated with almost all of the pixels zero"
    },
    {
      "start": 586.527,
      "duration": 3.539,
      "end": 590.066,
      "text": "except for some positive weights in this region that we care about,"
    },
    {
      "start": 590.066,
      "duration": 3.799,
      "end": 593.865,
      "text": "then taking the weighted sum of all the pixel values really just amounts"
    },
    {
      "start": 593.865,
      "duration": 3.955,
      "end": 597.82,
      "text": "to adding up the values of the pixel just in the region that we care about."
    },
    {
      "start": 599.14,
      "duration": 3.252,
      "end": 602.3919999999999,
      "text": "And if you really wanted to pick up on whether there's an edge here,"
    },
    {
      "start": 602.392,
      "duration": 4.208,
      "end": 606.6,
      "text": "what you might do is have some negative weights associated with the surrounding pixels."
    },
    {
      "start": 607.48,
      "duration": 2.557,
      "end": 610.037,
      "text": "Then the sum is largest when those middle pixels"
    },
    {
      "start": 610.037,
      "duration": 2.663,
      "end": 612.7,
      "text": "are bright but the surrounding pixels are darker."
    },
    {
      "start": 614.26,
      "duration": 4.387,
      "end": 618.6469999999999,
      "text": "When you compute a weighted sum like this, you might come out with any number,"
    },
    {
      "start": 618.647,
      "duration": 4.893,
      "end": 623.5400000000001,
      "text": "but for this network what we want is for activations to be some value between 0 and 1."
    },
    {
      "start": 624.12,
      "duration": 4.126,
      "end": 628.246,
      "text": "So a common thing to do is to pump this weighted sum into some function"
    },
    {
      "start": 628.246,
      "duration": 3.894,
      "end": 632.14,
      "text": "that squishes the real number line into the range between 0 and 1."
    },
    {
      "start": 632.46,
      "duration": 3.373,
      "end": 635.8330000000001,
      "text": "And a common function that does this is called the sigmoid function,"
    },
    {
      "start": 635.833,
      "duration": 1.587,
      "end": 637.42,
      "text": "also known as a logistic curve."
    },
    {
      "start": 638.0,
      "duration": 5.351,
      "end": 643.351,
      "text": "Basically very negative inputs end up close to 0, positive inputs end up close to 1,"
    },
    {
      "start": 643.351,
      "duration": 3.249,
      "end": 646.6,
      "text": "and it just steadily increases around the input 0."
    },
    {
      "start": 649.12,
      "duration": 3.517,
      "end": 652.6370000000001,
      "text": "So the activation of the neuron here is basically a"
    },
    {
      "start": 652.637,
      "duration": 3.723,
      "end": 656.3599999999999,
      "text": "measure of how positive the relevant weighted sum is."
    },
    {
      "start": 657.54,
      "duration": 2.101,
      "end": 659.641,
      "text": "But maybe it's not that you want the neuron to"
    },
    {
      "start": 659.641,
      "duration": 2.239,
      "end": 661.88,
      "text": "light up when the weighted sum is bigger than 0."
    },
    {
      "start": 662.28,
      "duration": 4.08,
      "end": 666.36,
      "text": "Maybe you only want it to be active when the sum is bigger than say 10."
    },
    {
      "start": 666.84,
      "duration": 3.42,
      "end": 670.26,
      "text": "That is, you want some bias for it to be inactive."
    },
    {
      "start": 671.38,
      "duration": 4.086,
      "end": 675.466,
      "text": "What we'll do then is just add in some other number like negative 10 to this"
    },
    {
      "start": 675.466,
      "duration": 4.194,
      "end": 679.66,
      "text": "weighted sum before plugging it through the sigmoid squishification function."
    },
    {
      "start": 680.58,
      "duration": 1.86,
      "end": 682.44,
      "text": "That additional number is called the bias."
    },
    {
      "start": 683.46,
      "duration": 3.85,
      "end": 687.3100000000001,
      "text": "So the weights tell you what pixel pattern this neuron in the second"
    },
    {
      "start": 687.31,
      "duration": 3.907,
      "end": 691.217,
      "text": "layer is picking up on, and the bias tells you how high the weighted"
    },
    {
      "start": 691.217,
      "duration": 3.963,
      "end": 695.18,
      "text": "sum needs to be before the neuron starts getting meaningfully active."
    },
    {
      "start": 696.12,
      "duration": 1.56,
      "end": 697.68,
      "text": "And that is just one neuron."
    },
    {
      "start": 698.28,
      "duration": 4.197,
      "end": 702.477,
      "text": "Every other neuron in this layer is going to be connected to"
    },
    {
      "start": 702.477,
      "duration": 4.196,
      "end": 706.673,
      "text": "all 784 pixel neurons from the first layer, and each one of"
    },
    {
      "start": 706.673,
      "duration": 4.267,
      "end": 710.94,
      "text": "those 784 connections has its own weight associated with it."
    },
    {
      "start": 711.6,
      "duration": 2.975,
      "end": 714.575,
      "text": "Also, each one has some bias, some other number that you add"
    },
    {
      "start": 714.575,
      "duration": 3.025,
      "end": 717.6,
      "text": "on to the weighted sum before squishing it with the sigmoid."
    },
    {
      "start": 718.11,
      "duration": 1.43,
      "end": 719.54,
      "text": "And that's a lot to think about!"
    },
    {
      "start": 719.96,
      "duration": 6.238,
      "end": 726.1980000000001,
      "text": "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights,"
    },
    {
      "start": 726.198,
      "duration": 1.782,
      "end": 727.98,
      "text": "along with 16 biases."
    },
    {
      "start": 728.84,
      "duration": 3.1,
      "end": 731.94,
      "text": "And all of that is just the connections from the first layer to the second."
    },
    {
      "start": 732.52,
      "duration": 2.363,
      "end": 734.883,
      "text": "The connections between the other layers also have"
    },
    {
      "start": 734.883,
      "duration": 2.457,
      "end": 737.34,
      "text": "a bunch of weights and biases associated with them."
    },
    {
      "start": 738.34,
      "duration": 5.46,
      "end": 743.8000000000001,
      "text": "All said and done, this network has almost exactly 13,000 total weights and biases."
    },
    {
      "start": 743.8,
      "duration": 3.265,
      "end": 747.0649999999999,
      "text": "13,000 knobs and dials that can be tweaked and turned"
    },
    {
      "start": 747.065,
      "duration": 2.895,
      "end": 749.96,
      "text": "to make this network behave in different ways."
    },
    {
      "start": 751.04,
      "duration": 3.222,
      "end": 754.262,
      "text": "So when we talk about learning, what that's referring to is"
    },
    {
      "start": 754.262,
      "duration": 3.385,
      "end": 757.6469999999999,
      "text": "getting the computer to find a valid setting for all of these"
    },
    {
      "start": 757.647,
      "duration": 3.713,
      "end": 761.36,
      "text": "many many numbers so that it'll actually solve the problem at hand."
    },
    {
      "start": 762.62,
      "duration": 4.566,
      "end": 767.186,
      "text": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting"
    },
    {
      "start": 767.186,
      "duration": 3.044,
      "end": 770.23,
      "text": "down and setting all of these weights and biases by hand,"
    },
    {
      "start": 770.23,
      "duration": 4.093,
      "end": 774.323,
      "text": "purposefully tweaking the numbers so that the second layer picks up on edges,"
    },
    {
      "start": 774.323,
      "duration": 2.257,
      "end": 776.5799999999999,
      "text": "the third layer picks up on patterns, etc."
    },
    {
      "start": 776.98,
      "duration": 4.362,
      "end": 781.342,
      "text": "I personally find this satisfying rather than just treating the network as a total black"
    },
    {
      "start": 781.342,
      "duration": 3.47,
      "end": 784.812,
      "text": "box, because when the network doesn't perform the way you anticipate,"
    },
    {
      "start": 784.812,
      "duration": 4.213,
      "end": 789.025,
      "text": "if you've built up a little bit of a relationship with what those weights and biases"
    },
    {
      "start": 789.025,
      "duration": 4.065,
      "end": 793.09,
      "text": "actually mean, you have a starting place for experimenting with how to change the"
    },
    {
      "start": 793.09,
      "duration": 1.09,
      "end": 794.1800000000001,
      "text": "structure to improve."
    },
    {
      "start": 794.96,
      "duration": 3.473,
      "end": 798.433,
      "text": "Or when the network does work but not for the reasons you might expect,"
    },
    {
      "start": 798.433,
      "duration": 3.816,
      "end": 802.249,
      "text": "digging into what the weights and biases are doing is a good way to challenge"
    },
    {
      "start": 802.249,
      "duration": 3.571,
      "end": 805.82,
      "text": "your assumptions and really expose the full space of possible solutions."
    },
    {
      "start": 806.84,
      "duration": 3.123,
      "end": 809.9630000000001,
      "text": "By the way, the actual function here is a little cumbersome to write down,"
    },
    {
      "start": 809.963,
      "duration": 0.717,
      "end": 810.68,
      "text": "don't you think?"
    },
    {
      "start": 812.5,
      "duration": 4.64,
      "end": 817.14,
      "text": "So let me show you a more notationally compact way that these connections are represented."
    },
    {
      "start": 817.66,
      "duration": 2.86,
      "end": 820.52,
      "text": "This is how you'd see it if you choose to read up more about neural networks."
    },
    {
      "start": 820.52,
      "duration": 7.503,
      "end": 828.023,
      "text": "Organize all of the activations from one layer into a column as a vector."
    },
    {
      "start": 828.357,
      "duration": 1.681,
      "end": 830.038,
      "text": "Then organize all of the weights as a matrix, where each row of that matrix corresponds"
    },
    {
      "start": 830.038,
      "duration": 7.962,
      "end": 838.0,
      "text": "to the connections between one layer and a particular neuron in the next layer."
    },
    {
      "start": 838.54,
      "duration": 3.674,
      "end": 842.2139999999999,
      "text": "What that means is that taking the weighted sum of the activations in"
    },
    {
      "start": 842.214,
      "duration": 3.673,
      "end": 845.8870000000001,
      "text": "the first layer according to these weights corresponds to one of the"
    },
    {
      "start": 845.887,
      "duration": 3.993,
      "end": 849.88,
      "text": "terms in the matrix vector product of everything we have on the left here."
    },
    {
      "start": 854.0,
      "duration": 3.714,
      "end": 857.714,
      "text": "By the way, so much of machine learning just comes down to having a good"
    },
    {
      "start": 857.714,
      "duration": 3.405,
      "end": 861.119,
      "text": "grasp of linear algebra, so for any of you who want a nice visual"
    },
    {
      "start": 861.119,
      "duration": 3.715,
      "end": 864.8340000000001,
      "text": "understanding for matrices and what matrix vector multiplication means,"
    },
    {
      "start": 864.834,
      "duration": 3.766,
      "end": 868.5999999999999,
      "text": "take a look at the series I did on linear algebra, especially chapter 3."
    },
    {
      "start": 869.24,
      "duration": 4.353,
      "end": 873.593,
      "text": "Back to our expression, instead of talking about adding the bias to each one of"
    },
    {
      "start": 873.593,
      "duration": 4.409,
      "end": 878.002,
      "text": "these values independently, we represent it by organizing all those biases into"
    },
    {
      "start": 878.002,
      "duration": 4.298,
      "end": 882.3,
      "text": "a vector, and adding the entire vector to the previous matrix vector product."
    },
    {
      "start": 883.28,
      "duration": 3.534,
      "end": 886.814,
      "text": "Then as a final step, I'll wrap a sigmoid around the outside here,"
    },
    {
      "start": 886.814,
      "duration": 3.856,
      "end": 890.67,
      "text": "and what that's supposed to represent is that you're going to apply the"
    },
    {
      "start": 890.67,
      "duration": 4.07,
      "end": 894.74,
      "text": "sigmoid function to each specific component of the resulting vector inside."
    },
    {
      "start": 895.94,
      "duration": 4.538,
      "end": 900.4780000000001,
      "text": "So once you write down this weight matrix and these vectors as their own symbols,"
    },
    {
      "start": 900.478,
      "duration": 4.93,
      "end": 905.4079999999999,
      "text": "you can communicate the full transition of activations from one layer to the next in an"
    },
    {
      "start": 905.408,
      "duration": 4.93,
      "end": 910.338,
      "text": "extremely tight and neat little expression, and this makes the relevant code both a lot"
    },
    {
      "start": 910.338,
      "duration": 4.426,
      "end": 914.764,
      "text": "simpler and a lot faster, since many libraries optimize the heck out of matrix"
    },
    {
      "start": 914.764,
      "duration": 0.896,
      "end": 915.66,
      "text": "multiplication."
    },
    {
      "start": 917.82,
      "duration": 3.64,
      "end": 921.46,
      "text": "Remember how earlier I said these neurons are simply things that hold numbers?"
    },
    {
      "start": 922.22,
      "duration": 5.11,
      "end": 927.33,
      "text": "Well of course the specific numbers that they hold depends on the image you feed in,"
    },
    {
      "start": 927.33,
      "duration": 4.258,
      "end": 931.5880000000001,
      "text": "so it's actually more accurate to think of each neuron as a function,"
    },
    {
      "start": 931.588,
      "duration": 5.292,
      "end": 936.88,
      "text": "one that takes in the outputs of all the neurons in the previous layer and spits out a"
    },
    {
      "start": 936.88,
      "duration": 1.46,
      "end": 938.34,
      "text": "number between 0 and 1."
    },
    {
      "start": 939.2,
      "duration": 3.93,
      "end": 943.13,
      "text": "Really the entire network is just a function, one that takes in"
    },
    {
      "start": 943.13,
      "duration": 3.93,
      "end": 947.06,
      "text": "784 numbers as an input and spits out 10 numbers as an output."
    },
    {
      "start": 947.56,
      "duration": 3.902,
      "end": 951.462,
      "text": "It's an absurdly complicated function, one that involves 13,000 parameters"
    },
    {
      "start": 951.462,
      "duration": 3.954,
      "end": 955.4159999999999,
      "text": "in the forms of these weights and biases that pick up on certain patterns,"
    },
    {
      "start": 955.416,
      "duration": 3.849,
      "end": 959.2650000000001,
      "text": "and which involves iterating many matrix vector products and the sigmoid"
    },
    {
      "start": 959.265,
      "duration": 3.375,
      "end": 962.64,
      "text": "squishification function, but it's just a function nonetheless."
    },
    {
      "start": 963.4,
      "duration": 3.26,
      "end": 966.66,
      "text": "And in a way it's kind of reassuring that it looks complicated."
    },
    {
      "start": 967.34,
      "duration": 2.361,
      "end": 969.701,
      "text": "I mean if it were any simpler, what hope would we have"
    },
    {
      "start": 969.701,
      "duration": 2.579,
      "end": 972.28,
      "text": "that it could take on the challenge of recognizing digits?"
    },
    {
      "start": 973.34,
      "duration": 1.36,
      "end": 974.7,
      "text": "And how does it take on that challenge?"
    },
    {
      "start": 975.08,
      "duration": 4.28,
      "end": 979.36,
      "text": "How does this network learn the appropriate weights and biases just by looking at data?"
    },
    {
      "start": 980.14,
      "duration": 3.054,
      "end": 983.194,
      "text": "Well that's what I'll show in the next video, and I'll also dig a little"
    },
    {
      "start": 983.194,
      "duration": 2.926,
      "end": 986.12,
      "text": "more into what this particular network we're seeing is really doing."
    },
    {
      "start": 987.58,
      "duration": 3.168,
      "end": 990.748,
      "text": "Now is the point I suppose I should say subscribe to stay notified"
    },
    {
      "start": 990.748,
      "duration": 2.4,
      "end": 993.148,
      "text": "about when that video or any new videos come out,"
    },
    {
      "start": 993.148,
      "duration": 4.272,
      "end": 997.4200000000001,
      "text": "but realistically most of you don't actually receive notifications from YouTube, do you?"
    },
    {
      "start": 998.02,
      "duration": 3.256,
      "end": 1001.276,
      "text": "Maybe more honestly I should say subscribe so that the neural networks"
    },
    {
      "start": 1001.276,
      "duration": 3.302,
      "end": 1004.578,
      "text": "that underlie YouTube's recommendation algorithm are primed to believe"
    },
    {
      "start": 1004.578,
      "duration": 3.302,
      "end": 1007.88,
      "text": "that you want to see content from this channel get recommended to you."
    },
    {
      "start": 1008.56,
      "duration": 1.38,
      "end": 1009.9399999999999,
      "text": "Anyway, stay posted for more."
    },
    {
      "start": 1010.76,
      "duration": 2.74,
      "end": 1013.5,
      "text": "Thank you very much to everyone supporting these videos on Patreon."
    },
    {
      "start": 1014.0,
      "duration": 3.439,
      "end": 1017.439,
      "text": "I've been a little slow to progress in the probability series this summer,"
    },
    {
      "start": 1017.439,
      "duration": 2.277,
      "end": 1019.716,
      "text": "but I'm jumping back into it after this project,"
    },
    {
      "start": 1019.716,
      "duration": 2.184,
      "end": 1021.9,
      "text": "so patrons you can look out for updates there."
    },
    {
      "start": 1023.6,
      "duration": 3.49,
      "end": 1027.09,
      "text": "To close things off here I have with me Lisha Li who did her PhD work on the"
    },
    {
      "start": 1027.09,
      "duration": 3.627,
      "end": 1030.7169999999999,
      "text": "theoretical side of deep learning and who currently works at a venture capital"
    },
    {
      "start": 1030.717,
      "duration": 3.903,
      "end": 1034.6200000000001,
      "text": "firm called Amplify Partners who kindly provided some of the funding for this video."
    },
    {
      "start": 1035.46,
      "duration": 3.66,
      "end": 1039.1200000000001,
      "text": "So Lisha one thing I think we should quickly bring up is this sigmoid function."
    },
    {
      "start": 1039.7,
      "duration": 3.458,
      "end": 1043.1580000000001,
      "text": "As I understand it early networks use this to squish the relevant weighted"
    },
    {
      "start": 1043.158,
      "duration": 3.364,
      "end": 1046.522,
      "text": "sum into that interval between zero and one, you know kind of motivated"
    },
    {
      "start": 1046.522,
      "duration": 3.318,
      "end": 1049.84,
      "text": "by this biological analogy of neurons either being inactive or active."
    },
    {
      "start": 1050.28,
      "duration": 3.76,
      "end": 1054.04,
      "text": "Exactly.\nBut relatively few modern networks actually use sigmoid anymore."
    },
    {
      "start": 1054.32,
      "duration": 1.44,
      "end": 1055.76,
      "text": "Yeah.\nIt's kind of old school right?"
    },
    {
      "start": 1055.76,
      "duration": 3.22,
      "end": 1058.98,
      "text": "Yeah or rather ReLU seems to be much easier to train."
    },
    {
      "start": 1059.4,
      "duration": 2.94,
      "end": 1062.3400000000001,
      "text": "And ReLU, ReLU stands for rectified linear unit?"
    },
    {
      "start": 1062.68,
      "duration": 4.721,
      "end": 1067.401,
      "text": "Yes it's this kind of function where you're just taking a max of zero"
    },
    {
      "start": 1067.401,
      "duration": 4.653,
      "end": 1072.054,
      "text": "and a where a is given by what you were explaining in the video and"
    },
    {
      "start": 1072.054,
      "duration": 4.516,
      "end": 1076.5700000000002,
      "text": "what this was sort of motivated from I think was a partially by a"
    },
    {
      "start": 1076.57,
      "duration": 4.79,
      "end": 1081.36,
      "text": "biological analogy with how neurons would either be activated or not."
    },
    {
      "start": 1081.36,
      "duration": 4.66,
      "end": 1086.02,
      "text": "And so if it passes a certain threshold it would be the identity function but if it did"
    },
    {
      "start": 1086.02,
      "duration": 4.82,
      "end": 1090.84,
      "text": "not then it would just not be activated so it'd be zero so it's kind of a simplification."
    },
    {
      "start": 1091.16,
      "duration": 4.535,
      "end": 1095.6950000000002,
      "text": "Using sigmoids didn't help training or it was very difficult to"
    },
    {
      "start": 1095.695,
      "duration": 4.534,
      "end": 1100.229,
      "text": "train at some point and people just tried ReLU and it happened"
    },
    {
      "start": 1100.229,
      "duration": 4.391,
      "end": 1104.6200000000001,
      "text": "to work very well for these incredibly deep neural networks."
    },
    {
      "start": 1105.1,
      "duration": 0.54,
      "end": 1105.6399999999999,
      "text": "All right thank you Lisha."
    }
  ]
}